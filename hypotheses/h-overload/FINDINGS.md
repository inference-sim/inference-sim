# H-Overload: 10x Overload Robustness

**Status:** Confirmed
**Resolution:** Clean confirmation
**Family:** Robustness/failure-mode
**VV&UQ:** Verification
**Tier:** 1
**Type:** Deterministic
**Date:** 2026-02-21
**Rounds:** 2

## Hypothesis

> Under stress condition of 10x the saturation rate, the system should exhibit defined overload behavior (queue growth with always-admit, or rejection with token-bucket) and NOT exhibit undefined behavior (panic, deadlock, silent data loss). Conservation (INV-1: injected == completed + queued + running + rejected) must hold at all overload levels.

## Experiment Design

**Classification:** Deterministic (conservation is exact pass/fail)

**Configurations compared:**
- Always-admit at 5 rate levels: 1x (300), 2x (600), 4x (1200), 7x (2100), 10x (3000) req/s
  - CLI: `--admission-policy always-admit --routing-policy least-loaded --scheduler fcfs --priority-policy constant --num-instances 4 --num-requests 2000 --rate <RATE> --seed 42 --horizon 5000000`
- Token-bucket at 5 rate levels: same rates, same flags plus:
  - CLI: `--admission-policy token-bucket --token-bucket-capacity 80000 --token-bucket-refill-rate 160000`

**Controlled variables:**
- Model: meta-llama/llama-3.1-8b-instruct (beta coefficients [6910.42, 17.67, 2.84])
- Instances: 4, routing: least-loaded, scheduler: FCFS
- Seed: 42, horizon: 5,000,000 ticks (5 seconds)
- Max requests: 2000
- Default KV blocks: 1,000,000 (no KV pressure)

**Varied variable:** Arrival rate (1x through 10x saturation)

**Seeds:** 42 (deterministic invariant -- single seed sufficient)

**Saturation rate derivation:**
The 1x reference rate of 300 req/s is a high-load estimate, not a precise saturation point. Rough derivation: with default prompt=512 tokens (CLI default `--prompt-tokens 512`) and output=512 tokens, beta coefficients [6910.42, 17.67, 2.84] give a prefill step time of ~15,957 us and per-decode step of ~6,913 us. With batching (max_running_reqs=256 per instance), the per-step decode cost when the batch is full is 6910.42 + 2.84*256 = 7,637 us, yielding ~33,500 tokens/s per instance. Accounting for prefill overhead and queue dynamics, effective throughput is roughly 60-70 completed req/s per instance, or ~250-280 req/s for 4 instances. The 300 req/s reference slightly exceeds this, ensuring genuine overload behavior at 1x and above. The exact saturation point depends on request size distribution, batch dynamics, and prefill/decode mix -- 300 is intentionally conservative as a "high-load reference rate."

**Preconditions verified:**
- Binary builds and runs at 1x rate without errors
- Trace summary enabled (`--summarize-trace --trace-level decisions`)
- Stderr captured separately for panic detection
- Token bucket parameters calibrated: per-input-token cost model (sim/admission.go:45), mean prompt ~512 tokens, refill rate 160,000 tokens/s ~ 312 req/s capacity at 1x

## Results

### Part 1: Robustness (no panics, no deadlocks)

| Policy | Rate | Exit Code | Panic | Status |
|--------|------|-----------|-------|--------|
| always-admit | 1x | 0 | no | PASS |
| always-admit | 2x | 0 | no | PASS |
| always-admit | 4x | 0 | no | PASS |
| always-admit | 7x | 0 | no | PASS |
| always-admit | 10x | 0 | no | PASS |
| token-bucket | 1x | 0 | no | PASS |
| token-bucket | 2x | 0 | no | PASS |
| token-bucket | 4x | 0 | no | PASS |
| token-bucket | 7x | 0 | no | PASS |
| token-bucket | 10x | 0 | no | PASS |

**Result: 10/10 configurations exit cleanly with exit code 0, no panics detected.**

### Part 2: Conservation Invariant (INV-1)

| Policy | Rate | Decisions | Injected | Completed | Queued | Running | Rejected | Checks | Status |
|--------|------|-----------|----------|-----------|--------|---------|----------|--------|--------|
| always-admit | 1x | 1501 | 1500 | 146 | 331 | 1023 | 0 | 8/8 | PASS |
| always-admit | 2x | 2000 | 2000 | 170 | 807 | 1023 | 0 | 8/8 | PASS |
| always-admit | 4x | 2000 | 2000 | 173 | 804 | 1023 | 0 | 8/8 | PASS |
| always-admit | 7x | 2000 | 2000 | 169 | 808 | 1023 | 0 | 8/8 | PASS |
| always-admit | 10x | 2000 | 2000 | 170 | 807 | 1023 | 0 | 8/8 | PASS |
| token-bucket | 1x | 1501 | 1500 | 146 | 331 | 1023 | 0 | 8/8 | PASS |
| token-bucket | 2x | 2000 | 1420 | 231 | 167 | 1022 | 580 | 9/9 | PASS |
| token-bucket | 4x | 2000 | 949 | 365 | 0 | 584 | 1051 | 9/9 | PASS |
| token-bucket | 7x | 2000 | 702 | 363 | 0 | 339 | 1298 | 9/9 | PASS |
| token-bucket | 10x | 2000 | 596 | 343 | 0 | 253 | 1404 | 9/9 | PASS |

**Result: 84/84 invariant checks pass across 10 configurations, zero conservation violations.**

**Column definitions:**
- **Decisions**: `total_decisions` from the trace summary -- the number of requests that reached the admission decision point (= `len(trace.Admissions)`). This equals the number of requests generated by the workload generator, not a separate "generated" counter.
- **Injected**: `injected_requests` from JSON output -- a derived field computed as `completed + still_queued + still_running`. This counts requests that successfully progressed through the instance-level lifecycle.

**Off-by-1 at 1x rate (root-caused in Round 2):** At 1x rate (300 req/s), 1501 requests are generated (last request at tick 4,999,500, just under the 5,000,000 horizon). All 1501 reach admission and routing (trace records 1501 decisions). However, the last request's instance-level `QueuedEvent` is scheduled at tick ~5,002,898 (arrival time 4,999,500 + alpha-model queueing delay of ~3,398 ticks). The cluster event loop's horizon check (`c.clock > c.config.Horizon`) cuts off this QueuedEvent, so the request is never enqueued into WaitQ and never appears in completed/queued/running counts. Code path: `ArrivalEvent.Execute` (sim/event.go:31) computes `queued_delay = alpha0 + alpha1 * len(inputTokens)` = 1601 + 3.51 * ~512 = ~3398 ticks, scheduling QueuedEvent past horizon. The cluster loop (sim/cluster/cluster.go:168-169) checks `c.clock > c.config.Horizon` and breaks before processing it. This is expected horizon-boundary behavior, not a bookkeeping bug. The analyze.py `diff <= 1` tolerance correctly handles this edge case.

Conservation checks performed per configuration:
- INV-1a: cluster conservation (injected == completed + queued + running)
- INV-1b: per-instance conservation (4 instances)
- INV-1c: cross-instance consistency (sum of per-instance injected == cluster injected)
- INV-1d: trace pipeline consistency (admitted + rejected == total_decisions)
- INV-1e: full pipeline conservation (injected + rejected == total_decisions)
- INV-1f: rejected count consistency (anomaly counter == trace counter), when applicable

### Part 3: Behavioral Characterization

**Always-admit behavior (queue growth under overload):**

| Rate | Completed | Queued | Running | Incomplete | E2E Mean (ms) |
|------|-----------|--------|---------|------------|---------------|
| 1x | 146 | 331 | 1023 | 1354 | 2444.5 |
| 2x | 170 | 807 | 1023 | 1830 | 3147.6 |
| 4x | 173 | 804 | 1023 | 1827 | 3586.7 |
| 7x | 169 | 808 | 1023 | 1831 | 3729.9 |
| 10x | 170 | 807 | 1023 | 1830 | 3825.0 |

Observations:
- Running batch saturates at 1023 across all rates (4 instances x 256 max_running_reqs - 1)
- Completed requests plateau at ~170 regardless of arrival rate. **Horizon artifact:** This plateau is driven primarily by the 5-second horizon cutoff, not throughput saturation. With mean service time ~3.5s per request (prefill + 512 decode steps), only requests arriving early enough in the 5-second window can complete within the horizon. At all rates >= 1x, the system quickly fills its running batch (1023 slots), and only the earliest-arriving requests have time to finish. The ~170 completed count reflects how many requests can start and finish within a 5-second window given the service time distribution, not the system's steady-state throughput capacity.
- Queue grows proportionally to excess arrivals
- E2E mean increases from 2444ms (1x) to 3825ms (10x) -- higher queueing delay. E2E is measured only for completed requests (which are biased toward early arrivals with shorter queue waits).

**Token-bucket behavior (rejection under overload):**

| Rate | Admitted | Rejected | Reject % | Completed | E2E Mean (ms) |
|------|----------|----------|----------|-----------|---------------|
| 1x | 1500 | 0 | 0% | 146 | 2444.5 |
| 2x | 1420 | 580 | 29% | 231 | 2885.1 |
| 4x | 949 | 1051 | 53% | 365 | 3453.5 |
| 7x | 702 | 1298 | 65% | 363 | 3707.8 |
| 10x | 596 | 1404 | 70% | 343 | 3791.6 |

Observations:
- Rejection rate grows from 0% at 1x to 70% at 10x (expected: token demand exceeds refill)
- Token bucket admits more requests initially (bucket starts full at capacity=80000), then rate-limits
- At 4x-10x, queue is 0 -- token bucket prevents queue buildup by rejecting excess
- Completed requests are higher at 4x (365) than at 1x (146) because more total requests arrive within the horizon, and early admits from the full bucket complete within the time window

## Root Cause Analysis

The system handles 10x overload robustly because of three independent mechanisms:

**1. Bounded running batch (sim/simulator.go: makeRunningBatch).**
The batch formation respects `MaxRunningReqs` (default 256 per instance, 1024 total). Excess requests wait in the queue. There is no unbounded growth in the running set. (RCV-1: `sim/simulator.go` batch formation loop, `sim/batch.go` construction.)

**2. Event-driven architecture (sim/cluster/cluster.go: Run loop, lines 126-189).**
The DES event loop processes one event at a time. There is no concurrent state mutation. The horizon check at lines 163 and 169 (`if c.clock > c.config.Horizon { break }`) cleanly terminates processing. Pending events beyond the horizon are simply not processed -- no partial state corruption. (RCV-1: cluster.go event loop.)

**3. Token bucket admission control (sim/admission.go: Admit, lines 37-51).**
The per-input-token cost model provides a natural backpressure mechanism. When the bucket empties, requests are rejected immediately. Rejected requests never enter the routing pipeline, so they cannot cause downstream issues. The trace records the rejection, and the anomaly counter tracks it. (RCV-1: admission.go, cluster_event.go:124-126.)

**Direction explanation (RCV-3):**
- Always-admit: queue grows because arrivals exceed service capacity. The running batch is bounded by MaxRunningReqs, so excess arrivals accumulate in the wait queue. This is expected and safe -- the queue is a simple FIFO list with no size limit.
- Token-bucket: rejections increase because token demand (rate * mean_prompt_tokens) exceeds refill rate. At 10x, demand is 3000 * 512 = 1,536,000 tokens/s vs refill 160,000 tokens/s. The initial bucket capacity (80,000) provides a brief burst absorption, then the bucket stays near empty.

**Control experiment (RCV-4):**
The token-bucket at 1x rate (refill ~= demand) serves as the control: 0 rejections, identical behavior to always-admit. This confirms that the rejection mechanism activates only when demand exceeds capacity, not due to a systematic bug in the admission path.

## Devil's Advocate (RCV-5)

**If this is "Confirmed," argue why it might be Refuted:**

The experiment uses default KV blocks (1,000,000) -- far more than needed for 2000 requests. Under KV pressure (e.g., total-kv-blocks=500), H12 previously found a panic in the preemption path (preempt() on empty RunningBatch). The overload experiment does not stress the KV path, so panics under combined overload + KV pressure remain untested. Additionally, with max_running_reqs=256 and 4 instances, the running batch accommodates 1024 concurrent requests, which absorbs much of the overload. A single-instance scenario or lower max_running_reqs might expose different failure modes.

**If this is "Refuted," argue why it might be Confirmed:**

The core conservation invariant is structural -- it depends on the event loop and request lifecycle (queued -> running -> completed), not on load level. The DES processes events sequentially with no concurrent mutation. As long as every request enters the queue and either completes, remains queued, or remains running, conservation holds. The 84/84 check results across 10 configurations (including extreme 10x overload) provide strong evidence.

## Findings Classification

| Finding | Type | Action |
|---------|------|--------|
| INV-1 holds at all overload levels (1x-10x) for both admission policies | Confirmation | Documented here |
| No panics, deadlocks, or silent data loss at 10x overload | Confirmation | Documented here |
| Token-bucket rejection rate grows from 0% (1x) to 70% (10x) -- expected behavior | Confirmation | Documented here |
| Always-admit shows bounded queue growth (running batch saturates at max_running_reqs) | Confirmation | Documented here |
| Off-by-1 at 1x: alpha-model queueing delay pushes last request's QueuedEvent past horizon (root-caused Round 2) | Expected behavior | No action -- horizon boundary correctly cuts off unprocessable events; analyze.py tolerance handles it |
| Overload + KV pressure (combined stress) not tested | Scope limitation | Hypothesis issue for follow-up |

## Standards Audit

Findings checked against docs/standards/:
- [x] Any violations of existing rules? None found. R19 (livelock protection) was a concern for overload paths, but the event-driven architecture with horizon cutoff prevents livelock.
- [x] Any new rules needed? None.
- [x] Any new invariants needed? None -- INV-1 already covers this. The experiment provides additional evidence that INV-1 holds under extreme conditions.
- [x] Any existing rules/invariants confirmed? INV-1 (request conservation -- 84/84 checks pass), R19 (livelock protection via event-driven architecture with horizon cutoff). INV-8 (work-conserving): the running batch staying saturated at 1023 is *consistent with* work-conserving behavior, but does not constitute a per-step proof. INV-8 requires that after every step completion, if `WaitQ.Len() > 0`, a `StepEvent` exists in the event queue. The batch saturation observation shows the system does not idle at the macro level, but does not verify the per-step invariant directly.

## Scope and Limitations (RCV-6)

- **Operating point tested:** 4 instances, least-loaded routing, FCFS scheduler, default KV (1M blocks), rates 300-3000 req/s, 5-second horizon, seed 42
- **Parameters findings depend on:** Conservation invariant is structural (event loop property). Behavioral characterization (queue growth rates, rejection percentages) depends on specific rate/capacity parameters.
- **What was NOT tested:**
  - KV-constrained scenarios (total-kv-blocks < 1000) -- known to trigger preemption cascades (H12)
  - Non-FCFS schedulers (SJF, priority-fcfs) under overload
  - Non-least-loaded routing (round-robin, weighted) under overload
  - Tiered KV cache under overload
  - Single-instance overload (different behavior: no routing pipeline)
  - Rates beyond 10x
- **Generalizability:** Conservation invariant likely holds at any overload level (structural property). Behavioral characterization is specific to tested parameters.
- **Uncertainty quantification:** UQ not applicable -- deterministic invariant, single seed sufficient. Conservation is exact pass/fail.

## Evidence Quality

| Metric | Value | Confidence |
|--------|-------|------------|
| Conservation (INV-1) | 84/84 checks pass | High -- exact pass/fail, no statistical noise |
| Robustness (no panics) | 10/10 configs clean | High -- exit code 0 + no panic in stderr |
| Rejection rate at 10x | 70% | High -- deterministic, consistent with token bucket math |
| Queue growth pattern | Running saturates at 1023 | High -- consistent with 4 * 256 - 1 |
| Mechanism | Event-driven + bounded batch + admission control | High -- traced through code (RCV-1) |

## Implications for Users

1. **BLIS handles extreme overload gracefully**: Even at 10x saturation rate, the simulator does not panic, deadlock, or lose requests. All requests are accounted for (completed, queued, running, or rejected).

2. **Token bucket provides effective overload protection**: At 10x overload, 70% of requests are rejected, preventing unbounded queue growth. The remaining admitted requests complete normally.

3. **Always-admit is safe but unbounded**: Under overload, the wait queue grows without limit. For capacity planning, users should either use a finite horizon or enable token-bucket admission to prevent unbounded queue accumulation.

4. **Running batch saturation**: The running batch saturates at `num_instances * max_running_reqs`. Beyond this, additional arrivals queue. This is the natural backpressure point in the system.

5. **KV pressure caveat**: These results were obtained with abundant KV cache (1,000,000 blocks per instance). Under KV-constrained configurations (e.g., `--total-kv-blocks 500`), the preemption code path activates and may exhibit different behavior under overload. H12 previously found a panic in the preemption path with KV pressure. Combined overload + KV pressure is an untested interaction that may produce different failure modes (see Scope and Limitations).

## Reproducing

```bash
cd hypotheses/h-overload
./run.sh
```
