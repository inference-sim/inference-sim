2026-01-20 10:08:43,120 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-20 10:08:48,685 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-20 10:08:48,687 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/codellama/CodeLlama-34b-Instruct-hf', 'model': '/model-cache/models/codellama/CodeLlama-34b-Instruct-hf', 'max_model_len': 4096, 'served_model_name': ['codellama/CodeLlama-34b-Instruct-hf'], 'otlp_traces_endpoint': 'http://otel-jan20-prefill-llama7b-tp1-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-20 10:08:50,703 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-20 10:08:53,595 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-20 10:08:53,596 INFO vllm.config.model: Using max model len 4096
2026-01-20 10:08:53,870 INFO vllm.config.scheduler: Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-01-20 10:08:56,067 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-20 10:08:58,387 INFO vllm.v1.engine.core: Waiting for init message from front-end.
2026-01-20 10:08:58,394 INFO vllm.v1.engine.core: Initializing a V1 LLM engine (v0.11.0) with config: model='/model-cache/models/codellama/CodeLlama-34b-Instruct-hf', speculative_config=None, tokenizer='/model-cache/models/codellama/CodeLlama-34b-Instruct-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint='http://otel-jan20-prefill-llama7b-tp1-1:4318/v1/traces', collect_detailed_traces=None), seed=0, served_model_name=codellama/CodeLlama-34b-Instruct-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
2026-01-20 10:08:59,936 INFO vllm.distributed.parallel_state: rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
2026-01-20 10:09:00,175 INFO vllm.v1.sample.ops.topk_topp_sampler: Using FlashInfer for top-p & top-k sampling.
2026-01-20 10:09:00,221 INFO vllm.v1.worker.gpu_model_runner: Starting to load model /model-cache/models/codellama/CodeLlama-34b-Instruct-hf...
2026-01-20 10:09:00,390 INFO vllm.v1.worker.gpu_model_runner: Loading model from scratch...
2026-01-20 10:09:00,445 INFO vllm.platforms.cuda: Using Flash Attention backend on V1 engine.
2026-01-20 10:09:58,239 INFO vllm.model_executor.model_loader.default_loader: Loading weights took 57.66 seconds
2026-01-20 10:09:58,570 INFO vllm.v1.worker.gpu_model_runner: Model loading took 62.8571 GiB and 57.854967 seconds
2026-01-20 10:10:03,426 INFO vllm.compilation.backends: Using cache directory: /root/.cache/vllm/torch_compile_cache/4423718a02/rank_0_0/backbone for vLLM's torch.compile
2026-01-20 10:10:03,426 INFO vllm.compilation.backends: Dynamo bytecode transform time: 4.73 s
2026-01-20 10:10:05,171 INFO vllm.compilation.backends: Cache the graph for dynamic shape for later use
2026-01-20 10:10:19,571 INFO vllm.compilation.backends: Compiling a graph for dynamic shape takes 15.89 s
2026-01-20 10:10:25,851 INFO vllm.compilation.monitor: torch.compile takes 20.63 s in total
2026-01-20 10:10:26,493 INFO vllm.v1.worker.gpu_worker: Available KV cache memory: 7.43 GiB
2026-01-20 10:10:26,711 INFO vllm.v1.core.kv_cache_utils: GPU KV cache size: 40,592 tokens
2026-01-20 10:10:26,711 INFO vllm.v1.core.kv_cache_utils: Maximum concurrency for 4,096 tokens per request: 9.91x
2026-01-20 10:10:33,345 INFO vllm.v1.worker.gpu_model_runner: Graph capturing finished in 6 secs, took 0.03 GiB
2026-01-20 10:10:33,376 INFO vllm.v1.engine.core: init engine (profile, create kv cache, warmup model) took 34.80 seconds
2026-01-20 10:10:33,658 INFO vllm.v1.metrics.loggers: Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 2537
2026-01-20 10:10:33,761 INFO vllm.entrypoints.openai.api_server: Supported_tasks: ['generate']
2026-01-20 10:10:33,762 INFO vllm.entrypoints.openai.api_server: Starting vLLM API server 0 on http://0.0.0.0:8000
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Available routes are:
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /openapi.json, Methods: HEAD, GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /docs, Methods: HEAD, GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /docs/oauth2-redirect, Methods: HEAD, GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /redoc, Methods: HEAD, GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /health, Methods: GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /load, Methods: GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /ping, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /ping, Methods: GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /tokenize, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /detokenize, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/models, Methods: GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /version, Methods: GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/responses, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/responses/{response_id}, Methods: GET
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/chat/completions, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/completions, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/embeddings, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /pooling, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /classify, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /score, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/score, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/audio/transcriptions, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/audio/translations, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /rerank, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v1/rerank, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /v2/rerank, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /scale_elastic_ep, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /is_scaling_elastic_ep, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /invocations, Methods: POST
2026-01-20 10:10:33,762 INFO vllm.entrypoints.launcher: Route: /metrics, Methods: GET
2026-01-20 10:11:34,014 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 2391.7 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.3%, Prefix cache hit rate: 30.0%
2026-01-20 10:11:44,014 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 2790.7 tokens/s, Avg generation throughput: 31.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 31.6%
2026-01-20 10:11:54,015 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 3811.1 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.6%, Prefix cache hit rate: 31.9%
2026-01-20 10:12:04,016 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 2222.6 tokens/s, Avg generation throughput: 33.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 32.9%
2026-01-20 10:12:07,895 ERROR vllm.entrypoints.openai.serving_completion: Error in preprocessing prompt inputs
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_completion.py", line 138, in create_completion
    engine_prompts = await renderer.render_prompt_and_embeds(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 260, in render_prompt_and_embeds
    token_prompts = await self.render_prompt(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 229, in render_prompt
    tokenized_text_prompts = await asyncio.gather(*tasks)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 330, in _tokenize
    return self._create_tokens_prompt(encoded.input_ids, max_length,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 385, in _create_tokens_prompt
    raise ValueError(
ValueError: This model's maximum context length is 4095 tokens. However, your request has 4176 input tokens. Please reduce the length of the input messages.
2026-01-20 10:12:14,017 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 8578.1 tokens/s, Avg generation throughput: 65.6 tokens/s, Running: 19 reqs, Waiting: 12 reqs, GPU KV cache usage: 90.7%, Prefix cache hit rate: 33.6%
2026-01-20 10:12:24,017 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 3978.9 tokens/s, Avg generation throughput: 64.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 33.5%
2026-01-20 10:12:34,018 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 8430.1 tokens/s, Avg generation throughput: 94.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.1%, Prefix cache hit rate: 33.2%
2026-01-20 10:12:44,020 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 4953.2 tokens/s, Avg generation throughput: 38.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.5%, Prefix cache hit rate: 33.2%
2026-01-20 10:12:54,020 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 8758.3 tokens/s, Avg generation throughput: 91.4 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 27.5%, Prefix cache hit rate: 33.1%
2026-01-20 10:13:03,241 ERROR vllm.entrypoints.openai.serving_completion: Error in preprocessing prompt inputs
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_completion.py", line 138, in create_completion
    engine_prompts = await renderer.render_prompt_and_embeds(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 260, in render_prompt_and_embeds
    token_prompts = await self.render_prompt(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 229, in render_prompt
    tokenized_text_prompts = await asyncio.gather(*tasks)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 330, in _tokenize
    return self._create_tokens_prompt(encoded.input_ids, max_length,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 385, in _create_tokens_prompt
    raise ValueError(
ValueError: This model's maximum context length is 4079 tokens. However, your request has 4199 input tokens. Please reduce the length of the input messages.
2026-01-20 10:13:03,762 ERROR vllm.entrypoints.openai.serving_completion: Error in preprocessing prompt inputs
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_completion.py", line 138, in create_completion
    engine_prompts = await renderer.render_prompt_and_embeds(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 260, in render_prompt_and_embeds
    token_prompts = await self.render_prompt(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 229, in render_prompt
    tokenized_text_prompts = await asyncio.gather(*tasks)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 330, in _tokenize
    return self._create_tokens_prompt(encoded.input_ids, max_length,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/renderer.py", line 385, in _create_tokens_prompt
    raise ValueError(
ValueError: This model's maximum context length is 4075 tokens. However, your request has 4214 input tokens. Please reduce the length of the input messages.
2026-01-20 10:13:04,020 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 7475.6 tokens/s, Avg generation throughput: 78.4 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 40.7%, Prefix cache hit rate: 33.1%
2026-01-20 10:13:14,020 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 6349.7 tokens/s, Avg generation throughput: 72.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 33.1%
2026-01-20 10:13:16,297 INFO vllm.entrypoints.launcher: Shutting down FastAPI HTTP server.
