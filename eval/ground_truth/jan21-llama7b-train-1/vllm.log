2026-01-21 09:12:34,961 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:12:40,476 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 09:12:40,478 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 09:12:42,476 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:12:45,493 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 09:12:51,117 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:12:56,563 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 09:12:56,569 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 09:12:58,552 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:13:01,576 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 09:13:23,452 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:13:28,966 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 09:13:28,968 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 09:13:30,981 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:13:34,007 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 09:14:00,494 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:14:05,977 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 09:14:05,979 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 09:14:07,983 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:14:10,926 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 09:14:56,457 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:15:01,971 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 09:15:01,973 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 09:15:03,961 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:15:06,918 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 09:16:37,488 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:16:42,953 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 09:16:42,955 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 09:16:44,950 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:16:47,937 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 09:19:40,445 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:19:45,884 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 09:19:45,886 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 09:19:47,861 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 09:19:50,865 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 10:44:43,414 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:44:49,501 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 10:44:49,503 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 10:44:51,634 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:44:54,863 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 10:45:01,528 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:45:07,370 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 10:45:07,372 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 10:45:09,499 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:45:12,616 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 10:45:34,486 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:45:40,496 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 10:45:40,499 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 10:45:42,597 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:45:45,989 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 10:46:14,254 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:46:20,191 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 10:46:20,193 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 10:46:22,261 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:46:25,364 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 10:47:14,504 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:47:20,324 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 10:47:20,326 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 8192, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 10:47:22,489 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:47:25,648 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 10:48:57,895 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:49:03,749 INFO vllm.entrypoints.openai.api_server: vLLM API server version 0.11.0
2026-01-21 10:49:03,751 INFO vllm.entrypoints.utils: non-default args: {'model_tag': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'model': '/model-cache/models/meta-llama/Llama-2-7b-hf', 'max_model_len': 4096, 'served_model_name': ['meta-llama/Llama-2-7b-hf'], 'otlp_traces_endpoint': 'http://otel-jan21-llama7b-train-1:4318/v1/traces', 'max_num_batched_tokens': 4096, 'max_num_seqs': 256}
2026-01-21 10:49:05,842 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:49:09,066 INFO vllm.config.model: Resolved architecture: LlamaForCausalLM
2026-01-21 10:49:09,066 INFO vllm.config.model: Using max model len 4096
2026-01-21 10:49:09,345 INFO vllm.config.scheduler: Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-01-21 10:49:11,779 INFO vllm.platforms: Automatically detected platform cuda.
2026-01-21 10:49:14,328 INFO vllm.v1.engine.core: Waiting for init message from front-end.
2026-01-21 10:49:14,336 INFO vllm.v1.engine.core: Initializing a V1 LLM engine (v0.11.0) with config: model='/model-cache/models/meta-llama/Llama-2-7b-hf', speculative_config=None, tokenizer='/model-cache/models/meta-llama/Llama-2-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint='http://otel-jan21-llama7b-train-1:4318/v1/traces', collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-2-7b-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
2026-01-21 10:49:15,997 INFO vllm.distributed.parallel_state: rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
2026-01-21 10:49:16,188 INFO vllm.v1.sample.ops.topk_topp_sampler: Using FlashInfer for top-p & top-k sampling.
2026-01-21 10:49:16,224 INFO vllm.v1.worker.gpu_model_runner: Starting to load model /model-cache/models/meta-llama/Llama-2-7b-hf...
2026-01-21 10:49:16,422 INFO vllm.v1.worker.gpu_model_runner: Loading model from scratch...
2026-01-21 10:49:16,483 INFO vllm.platforms.cuda: Using Flash Attention backend on V1 engine.
2026-01-21 10:49:28,750 INFO vllm.model_executor.model_loader.default_loader: Loading weights took 12.15 seconds
2026-01-21 10:49:29,153 INFO vllm.v1.worker.gpu_model_runner: Model loading took 12.5524 GiB and 12.333144 seconds
2026-01-21 10:49:32,751 INFO vllm.compilation.backends: Using cache directory: /root/.cache/vllm/torch_compile_cache/859996446d/rank_0_0/backbone for vLLM's torch.compile
2026-01-21 10:49:32,752 INFO vllm.compilation.backends: Dynamo bytecode transform time: 3.47 s
2026-01-21 10:49:34,647 INFO vllm.compilation.backends: Cache the graph for dynamic shape for later use
2026-01-21 10:49:45,408 INFO vllm.compilation.backends: Compiling a graph for dynamic shape takes 12.46 s
2026-01-21 10:49:48,320 INFO vllm.compilation.monitor: torch.compile takes 15.93 s in total
2026-01-21 10:49:48,905 INFO vllm.v1.worker.gpu_worker: Available KV cache memory: 58.12 GiB
2026-01-21 10:49:49,115 INFO vllm.v1.core.kv_cache_utils: GPU KV cache size: 119,008 tokens
2026-01-21 10:49:49,115 INFO vllm.v1.core.kv_cache_utils: Maximum concurrency for 4,096 tokens per request: 29.05x
2026-01-21 10:49:53,215 INFO vllm.v1.worker.gpu_model_runner: Graph capturing finished in 3 secs, took 0.12 GiB
2026-01-21 10:49:53,224 INFO vllm.v1.engine.core: init engine (profile, create kv cache, warmup model) took 24.07 seconds
2026-01-21 10:49:53,510 INFO vllm.v1.metrics.loggers: Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 7438
2026-01-21 10:49:53,589 INFO vllm.entrypoints.openai.api_server: Supported_tasks: ['generate']
2026-01-21 10:49:53,590 WARNING vllm.config.model: Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-01-21 10:49:53,590 INFO vllm.entrypoints.openai.serving_responses: Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
2026-01-21 10:49:53,590 INFO vllm.entrypoints.openai.serving_chat: Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
2026-01-21 10:49:53,590 INFO vllm.entrypoints.openai.serving_completion: Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
2026-01-21 10:49:53,590 INFO vllm.entrypoints.openai.api_server: Starting vLLM API server 0 on http://0.0.0.0:8000
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Available routes are:
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /openapi.json, Methods: HEAD, GET
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /docs, Methods: HEAD, GET
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /docs/oauth2-redirect, Methods: HEAD, GET
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /redoc, Methods: HEAD, GET
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /health, Methods: GET
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /load, Methods: GET
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /ping, Methods: POST
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /ping, Methods: GET
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /tokenize, Methods: POST
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /detokenize, Methods: POST
2026-01-21 10:49:53,590 INFO vllm.entrypoints.launcher: Route: /v1/models, Methods: GET
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /version, Methods: GET
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/responses, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/responses/{response_id}, Methods: GET
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/chat/completions, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/completions, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/embeddings, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /pooling, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /classify, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /score, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/score, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/audio/transcriptions, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/audio/translations, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /rerank, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v1/rerank, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /v2/rerank, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /scale_elastic_ep, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /is_scaling_elastic_ep, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /invocations, Methods: POST
2026-01-21 10:49:53,591 INFO vllm.entrypoints.launcher: Route: /metrics, Methods: GET
2026-01-21 10:50:53,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 152.7 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:51:03,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 560.1 tokens/s, Avg generation throughput: 146.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
2026-01-21 10:51:13,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 646.5 tokens/s, Avg generation throughput: 147.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
2026-01-21 10:51:23,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1123.5 tokens/s, Avg generation throughput: 147.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
2026-01-21 10:51:33,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 329.2 tokens/s, Avg generation throughput: 151.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
2026-01-21 10:51:43,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 893.3 tokens/s, Avg generation throughput: 146.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:51:53,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 636.1 tokens/s, Avg generation throughput: 89.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:52:03,855 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:52:23,856 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 3693.6 tokens/s, Avg generation throughput: 397.6 tokens/s, Running: 21 reqs, Waiting: 0 reqs, GPU KV cache usage: 34.4%, Prefix cache hit rate: 0.0%
2026-01-21 10:52:33,856 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 198.8 tokens/s, Avg generation throughput: 644.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
2026-01-21 10:52:43,857 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 533.3 tokens/s, Avg generation throughput: 201.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:52:53,857 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 856.4 tokens/s, Avg generation throughput: 200.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
2026-01-21 10:53:03,857 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 797.0 tokens/s, Avg generation throughput: 197.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
2026-01-21 10:53:13,857 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 694.0 tokens/s, Avg generation throughput: 202.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
2026-01-21 10:53:23,857 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1160.6 tokens/s, Avg generation throughput: 167.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:53:33,857 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 380.8 tokens/s, Avg generation throughput: 110.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:53:43,858 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 917.1 tokens/s, Avg generation throughput: 74.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 0.0%
2026-01-21 10:53:53,858 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 765.3 tokens/s, Avg generation throughput: 142.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
2026-01-21 10:54:03,858 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 753.1 tokens/s, Avg generation throughput: 208.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
2026-01-21 10:54:13,859 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1604.8 tokens/s, Avg generation throughput: 212.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
2026-01-21 10:54:23,860 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 422.8 tokens/s, Avg generation throughput: 213.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
2026-01-21 10:54:33,861 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 681.8 tokens/s, Avg generation throughput: 158.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:54:43,861 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 286.4 tokens/s, Avg generation throughput: 58.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
2026-01-21 10:54:53,861 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 584.0 tokens/s, Avg generation throughput: 214.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:55:03,861 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 872.6 tokens/s, Avg generation throughput: 162.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
2026-01-21 10:55:13,862 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 911.8 tokens/s, Avg generation throughput: 202.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.4%, Prefix cache hit rate: 0.0%
2026-01-21 10:55:23,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1071.0 tokens/s, Avg generation throughput: 174.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:55:33,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 652.2 tokens/s, Avg generation throughput: 208.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
2026-01-21 10:55:43,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 557.2 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:55:53,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 749.7 tokens/s, Avg generation throughput: 293.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
2026-01-21 10:56:03,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 678.0 tokens/s, Avg generation throughput: 233.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
2026-01-21 10:56:13,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 738.5 tokens/s, Avg generation throughput: 235.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:56:23,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 908.0 tokens/s, Avg generation throughput: 237.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
2026-01-21 10:56:33,863 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 387.8 tokens/s, Avg generation throughput: 182.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:56:43,864 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 486.1 tokens/s, Avg generation throughput: 174.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
2026-01-21 10:56:53,864 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1166.0 tokens/s, Avg generation throughput: 241.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:57:03,865 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1037.3 tokens/s, Avg generation throughput: 223.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%
2026-01-21 10:57:13,865 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 936.8 tokens/s, Avg generation throughput: 222.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 0.0%
2026-01-21 10:57:23,866 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 954.1 tokens/s, Avg generation throughput: 170.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
2026-01-21 10:57:33,867 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 476.4 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
2026-01-21 10:57:43,867 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1407.2 tokens/s, Avg generation throughput: 207.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:57:53,867 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 901.6 tokens/s, Avg generation throughput: 241.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
2026-01-21 10:58:03,867 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 857.3 tokens/s, Avg generation throughput: 240.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
2026-01-21 10:58:13,868 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 665.3 tokens/s, Avg generation throughput: 201.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:58:23,869 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 384.1 tokens/s, Avg generation throughput: 50.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:58:33,869 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:58:53,871 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 2374.1 tokens/s, Avg generation throughput: 620.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
2026-01-21 10:59:03,872 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1444.6 tokens/s, Avg generation throughput: 213.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.9%, Prefix cache hit rate: 0.0%
2026-01-21 10:59:13,872 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 364.1 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:59:23,873 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 155.2 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 10:59:33,873 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1516.5 tokens/s, Avg generation throughput: 237.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
2026-01-21 10:59:43,875 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1131.3 tokens/s, Avg generation throughput: 253.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
2026-01-21 10:59:53,875 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1012.8 tokens/s, Avg generation throughput: 283.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
2026-01-21 11:00:03,875 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1007.4 tokens/s, Avg generation throughput: 225.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%
2026-01-21 11:00:13,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 347.6 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 11:00:23,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1287.7 tokens/s, Avg generation throughput: 180.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 0.0%
2026-01-21 11:00:33,877 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 957.5 tokens/s, Avg generation throughput: 226.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 0.0%
2026-01-21 11:00:43,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1101.9 tokens/s, Avg generation throughput: 240.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.3%, Prefix cache hit rate: 0.0%
2026-01-21 11:00:53,877 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1021.4 tokens/s, Avg generation throughput: 240.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
2026-01-21 11:01:03,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 463.9 tokens/s, Avg generation throughput: 73.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
2026-01-21 11:01:13,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 621.8 tokens/s, Avg generation throughput: 181.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
2026-01-21 11:01:23,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1170.4 tokens/s, Avg generation throughput: 154.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
2026-01-21 11:01:33,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 1198.2 tokens/s, Avg generation throughput: 268.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
2026-01-21 11:01:43,876 INFO vllm.v1.metrics.loggers: Engine 000: Avg prompt throughput: 799.0 tokens/s, Avg generation throughput: 222.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
2026-01-21 11:01:49,341 INFO vllm.entrypoints.launcher: Shutting down FastAPI HTTP server.
