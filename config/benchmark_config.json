{
  "gpu_specs": {
    "H100": {
      "peak_tflops_fp16": 989.5,
      "peak_tflops_fp8": 1979.0,
      "peak_memory_bw_tbs": 3.35,
      "effective_memory_bw_factor": 0.8,
      "nvlink_bw_gbs": 900,
      "num_sms": 132,
      "memory_gb": 80
    }
  },
  "benchmark_configs": [
    {
      "name": "llama-2-7b",
      "num_attention_heads": 32,
      "num_key_value_heads": 32,
      "head_dim": 128,
      "hidden_size": 4096,
      "num_hidden_layers": 32,
      "intermediate_size": 11008,
      "description": "Llama-2-7B (MHA) - Default baseline model for benchmarks",
      "validation_model": "meta-llama/Llama-2-7b-hf"
    },
    {
      "name": "llama-2-70b",
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "head_dim": 128,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "intermediate_size": 28672,
      "description": "Llama-2-70B (GQA) - Larger model for performance comparison",
      "validation_model": "meta-llama/Llama-2-70b-hf"
    }
  ],
  "validation_configs": {
    "primary_models": [
      "meta-llama/Llama-2-7b-hf"
    ],
    "tp_values": [1],
    "prefill_test_sizes": [512, 768, 1024, 2048],
    "decode_batch_sizes": [1, 8, 16, 32, 64, 128],
    "workload_scenarios": {
      "general": {
        "description": "General chat workload",
        "prefill_tokens": 547,
        "output_tokens": 248,
        "peak_qps": 20.0
      },
      "codegen": {
        "description": "Code generation workload",
        "prefill_tokens": 566,
        "output_tokens": 247,
        "peak_qps": 10.0
      },
      "roleplay": {
        "description": "Roleplay/creative writing",
        "prefill_tokens": 750,
        "output_tokens": 251,
        "peak_qps": 6.0
      },
      "reasoning": {
        "description": "Long-form reasoning (decode-heavy)",
        "prefill_tokens": 1034,
        "output_tokens": 1448,
        "peak_qps": 4.0
      }
    }
  },
  "mfu_validation": {
    "H100": {
      "decode": {
        "min": 0.005,
        "max": 0.30,
        "description": "Memory-bound decode operations"
      },
      "prefill": {
        "min": 0.30,
        "max": 0.85,
        "description": "Compute-bound prefill operations"
      },
      "gemm": {
        "min": 0.05,
        "max": 0.85,
        "description": "GEMM operations vary with batch size (FP8)"
      }
    }
  },
  "gemm_sweep": {
    "H100": {
      "_comment": "M values are swept internally by deepgemm_gemm.py: [8, 16, 32, 64, 128, 256, 512, 1024, 4096, 8192, 16384, 32768, 64*1024, 128*1024]",
      "k_values": [2048, 3584, 4096, 8192],
      "n_values": [6144, 11008, 14336, 18944]
    }
  },
  "openshift": {
    "namespace": "diya",
    "job_name_prefix": "infersim",
    "container_image": "pytorch/pytorch:2.9.1-cuda12.8-cudnn9-devel",
    "gpu_resource": "nvidia.com/gpu",
    "node_selector": "nvidia-h100"
  }
}
