# BLIS Inference-Perf Workload Format Example
#
# This file demonstrates BLIS's native support for inference-perf style
# workload specifications. Instead of manually writing 45 client definitions,
# use the compact inference_perf block.
#
# ============================================================================
# INFERENCE-PERF TO BLIS TRANSLATION
# ============================================================================
#
# inference-perf:                       BLIS equivalent:
#   load.stages[].rate     -->          stages[].rate (req/s)
#   load.stages[].duration -->          stages[].duration (seconds)
#   data.shared_prefix.num_unique_system_prompts  --> N prefix groups
#   data.shared_prefix.num_users_per_system_prompt --> M clients per group
#   data.shared_prefix.system_prompt_len --> prefix_length on each client
#   data.shared_prefix.question_len     --> constant input distribution
#   data.shared_prefix.output_len       --> constant output distribution
#   data.shared_prefix.enable_multi_turn_chat --> reasoning.multi_turn
#
# ============================================================================
# TRY IT
# ============================================================================
#
#   ./blis run \
#     --model meta-llama/llama-3.1-8b-instruct \
#     --num-instances 4 --routing-policy weighted \
#     --workload-spec examples/inference-perf-shared-prefix.yaml
#
# This auto-generates 45 clients (9 prompts x 5 users), each sharing a
# 100-token system prompt prefix within their group.

version: "1"
seed: 42
aggregate_rate: 14.0   # Overridden by time-weighted average if omitted

inference_perf:
  stages:
    - rate: 8.0        # Ramp-up: 8 req/s for 10 minutes
      duration: 600
    - rate: 20.0       # Burst: 20 req/s for 10 minutes
      duration: 600

  shared_prefix:
    num_unique_system_prompts: 9
    num_users_per_system_prompt: 5
    system_prompt_len: 100     # 100-token shared prefix per group
    question_len: 447          # Fixed input length (constant distribution)
    output_len: 248            # Fixed output length
    enable_multi_turn_chat: false
